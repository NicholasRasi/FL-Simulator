#   Text classification for customer support processes
#
#   Financial or government institutions that wish to train a chatbot for their clients cannot
#   be allowed to upload all text data from the client-side to their central server due to strict
#   privacy protection statements.
#   Text classification can be used to automatically route support tickets to a teammate
#   with specific product expertise.
#
#   Sentiment classification in particular can be used to:
#   - Automatically detect the urgency of a support ticket and prioritize those that contain negative sentiments.
#   - Classify calls according to sentiment (i.e positive or negative) in order to help call centre quality
#     controllers monitor the performance of their agents, and get useful feedback about the satisfaction
#     of their customers.
#
#   The dataset considered is sentiment140 which contains twitter comments and the corresponding sentiment class.
#   It's assumed that the devices used for the training are smartphones and computers since those are the devices
#   more common when contacting chatbots.
#

simulation:
  # number or repetitions
  repetitions: 3
  # simulation output folder
  output_folder: "output"
  # simulation output file
  output_file: "feddyn_usecase3.json"
  # model
  model_name: "sentiment140"
  # metric
  metric: "accuracy"
  # maximum number of rounds
  num_rounds: 30
  # list of stopping conditions (OR) for ending the simulation
  stop_conds:
    # global model accuracy (during evaluation)
    metric: 1
    # global model loss (during evaluation)
    loss: 0
  # initialization policy
  initializer: "default"
  # fixing random state for reproducibility
  seed: 1
  # set the TensorFlow verbosity, 0 = silent, 1 = progress bar, 2 = one line per epoch
  tf_verbosity: 1
  # number of concurrent processes
  num_processes: 1

algorithms:
  federated_algorithm: "feddyn"
  fit:
    # algorithm used for the FL aggregation
    aggregation: "fedavg"
    # algorithm used for the clients selection
    selection: "random"
    # algorithm used for the update optimizer
    update: "static"
    # algorithm used for the local data optimizer
    data: "random"
    # parameters used by the algorithm
    params:
      # fraction of devices used for the computation
      k: 0.7
      # number of epochs executed for fit for each round
      epochs: 5
      # batch size used for fit for each round
      batch_size: 32
      # number of examples used for fit for each round
      num_examples: 500
  eval:
    selection: "random"
    update: "static"
    data: "random"
    params:
      k: 0.5
      epochs: 5
      batch_size: 32
      num_examples: 100
  # optimizer used (local iteration) string or optimizer instance
  optimizer: "sgd"

devices:
  # total number of devices (D)
  num: 30
  # probability a device is available for a round
  p_available: 1
  # probability a device fails during a round
  p_fail: 0.05
  # number of malicious devices
  adversary_num: 0

computation:
  # mean and variance of number of computed iterations/second per device (among devices) [iter/s]
  ips_mean: 100
  ips_var: 50

energy:
  # mean and variance of energy available at each device [mWh]
  avail_mean: 25000
  avail_var: 20000
  # power consumption for 1 second of computation [mW/s]
  pow_comp_s: 100
  # power consumption for 1 second of network used [mW/s]
  pow_net_s: 200

network:
  # mean and variance of network speed available at each device [params/s]
  speed_mean: 100000
  speed_var: 1000

data:
  # mean and variance of number of examples available at each device
  num_examples_mean: 1500
  num_examples_var: 0
  # use non-i.d.d data
  non_iid_partitions: 0.45
  # percentage of mislabelled data
  mislabelling_percentage: 0



